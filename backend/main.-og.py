from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import dotenv

# Load environment variables from a .env file
dotenv.load_dotenv()

from openai_service import aclient, generate_openai_response

# Create an instance of the FastAPI application
app = FastAPI()

# Configure CORS (Cross-Origin Resource Sharing) to allow requests from specified origins
origins = ["http://localhost:3000"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # List of allowed origins
    allow_credentials=True,  # Allow cookies to be included in cross-origin requests
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all HTTP headers
)

# Define the root endpoint
@app.get("/")
async def root():
    return {"message": "Hello, World"}  # Return a simple JSON response

# Define the endpoint to handle POST requests for OpenAI API
@app.post("/api/openai")
async def openai_stream(request: Request):
    # Parse the JSON body of the request
    data = await request.json()
    
    # Extract the list of messages from the request
    messages = data.get('messages', [])

    # Define the system message
    system_message = {"role": "system", "content": "You are a helpful assistant. Finish each sentence with sentiment in curly braces {}."}
    
    # Insert the system message at the beginning of the formatted messages list
    formatted_messages = [
        {"role": msg["sender"], "content": msg["text"]} for msg in messages
    ]
    formatted_messages.insert(0, system_message)

    # Return the response generated by the OpenAI service as a streaming response
    return StreamingResponse(generate_openai_response(formatted_messages), media_type='text/plain')

# Run the FastAPI application using Uvicorn if this file is executed as the main program
if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host='0.0.0.0', port=5000, reload=True)  # Host and port configuration
