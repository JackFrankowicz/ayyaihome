LLM_MODEL_CONFIG:
  OPENAI:
    RESPONSE_MODEL: "gpt-4o-mini"
    TEMPERATURE: 1.0
    TOP_P: 1.0
    N: 1
    SYSTEM_PROMPT_CONTENT: "You are a helpful but witty and dry assistant"
    STREAM_OPTIONS:
      include_usage: true
    STOP: null
    MAX_TOKENS: null
    PRESENCE_PENALTY: 0.0
    FREQUENCY_PENALTY: 0.0
    LOGIT_BIAS: null
    USER: null
    TOOLS: null
    TOOL_CHOICE: null
    MODALITIES:
      - text

  ANTHROPIC:
    RESPONSE_MODEL: "claude-3-5-haiku-20241022"
    TEMPERATURE: 1
    TOP_P: 1
    SYSTEM_PROMPT: "you are a dry and witty assistant"
    MAX_TOKENS: 1024
    STOP_SEQUENCES: null
    STREAM_OPTIONS:
      include_usage: true

  GEMINI:
    MODEL_VERSION: "gemini-1.5-flash"  # Specify the desired Gemini model version
    TEMPERATURE: 0.7  # Adjust the temperature for response randomness
    SYSTEM_PROMPT: "You are a knowledgeable assistant."  # Define the system instruction
    MAX_OUTPUT_TOKENS: 150  # Set the maximum number of tokens for the response
    TOP_P: 0.9  # Adjust the nucleus sampling parameter
    CANDIDATE_COUNT: 1  # Number of response candidates to generate