LLM_MODEL_CONFIG:
  OPENAI:
    RESPONSE_MODEL: "gpt-4o-mini"
    TEMPERATURE: 1.0
    TOP_P: 1.0
    N: 1
    SYSTEM_PROMPT_CONTENT: "You rhyme all your replies"
    STREAM_OPTIONS:
      include_usage: true
    STOP: null
    MAX_TOKENS: null
    PRESENCE_PENALTY: 0.0
    FREQUENCY_PENALTY: 0.0
    LOGIT_BIAS: null
    USER: null
    TOOLS: null
    TOOL_CHOICE: null
    MODALITIES:
      - text

  ANTHROPIC:
    RESPONSE_MODEL: "claude-3-5-haiku-20241022"
    TEMPERATURE: 1
    TOP_P: 1
    SYSTEM_PROMPT: "you are a dry and witty assistant"
    MAX_TOKENS: 1024
    STOP_SEQUENCES: null
    STREAM_OPTIONS:
      include_usage: true

  GEMINI:
    MODEL_VERSION: "gemini-1.5-flash"  # Specify the desired Gemini model version
    TEMPERATURE: 0.7  # Adjust the temperature for response randomness
    SYSTEM_PROMPT: "speak like yoda"  # Define the system instruction
    MAX_OUTPUT_TOKENS: 150  # Set the maximum number of tokens for the response
    TOP_P: 0.9  # Adjust the nucleus sampling parameter
    CANDIDATE_COUNT: 1  # Number of response candidates to generate

  MISTRAL:
    RESPONSE_MODEL: "mistral-tiny"  # Specify the desired Mistral model
    TEMPERATURE: 0.7  # Adjust the temperature for response randomness
    TOP_P: 0.9  # Adjust the nucleus sampling parameter
    SYSTEM_PROMPT: "You are a helpful and concise assistant."  # Define the system instruction
    MAX_TOKENS: 1500  # Set the maximum number of tokens for the response
    STOP_SEQUENCES: null
    STREAM_OPTIONS:
      include_usage: true

  GROK:
    API_BASE: "https://api.x.ai/v1"
    RESPONSE_MODEL: "grok-beta"  # Specify the desired Grok model
    TEMPERATURE: 0.7  # Adjust the temperature for response creativity
    TOP_P: 0.9  # Nucleus sampling for diversity
    SYSTEM_PROMPT: "You are a witty and creative assistant."  # Define the system instruction
    MAX_TOKENS: 2000  # Set the maximum number of tokens for the response
    STOP: null  # Define optional stop sequences
    STREAM_OPTIONS:
      include_usage: true  # Include usage details in the response

  DEEPINFRA:
    API_BASE: "https://api.deepinfra.com/v1/openai"
    RESPONSE_MODEL: "meta-llama/Meta-Llama-3.1-70B-Instruct"
    TEMPERATURE: 0.8  # Adjust temperature for response randomness
    TOP_P: 0.95  # Adjust the nucleus sampling parameter
    N: 1  # Number of completions to generate per prompt
    MAX_TOKENS: 2048  # Maximum number of tokens for the response
    PRESENCE_PENALTY: 0.0  # Penalize new tokens based on their presence
    FREQUENCY_PENALTY: 0.0  # Penalize new tokens based on their frequency
    LOGIT_BIAS: null  # Map for biasing logit probabilities
    USER: null  # Identifier for the end user
    STOP: null  # Specify optional stop sequences
    SYSTEM_PROMPT: "You are a precise and friendly assistant."  # Define the system instruction
    STREAM_OPTIONS:
      include_usage: true  # Include usage details in the response
      
  OPENROUTER:
    API_BASE: "https://openrouter.ai/api/v1"
    RESPONSE_MODEL: "gryphe/mythomax-l2-13b:extended"  # Specify the desired OpenRouter model
    TEMPERATURE: 1.0  # Adjust the temperature for response randomness
    TOP_P: 1.0  # Adjust the nucleus sampling parameter
    N: 1.0  # Number of completions to generate per prompt
    MAX_TOKENS: 2048  # Maximum number of tokens for the response
    PRESENCE_PENALTY: 0.0  # Penalize new tokens based on their presence
    FREQUENCY_PENALTY: 0.0  # Penalize new tokens based on their frequency
    LOGIT_BIAS: null  # Map for biasing logit probabilities
    USER: null  # Identifier for the end user
    STOP: null  # Specify optional stop sequences
    SYSTEM_PROMPT: "swear as much as possible"  # Define the system instruction
    STREAM_OPTIONS:
      include_usage: true  # Include usage details in the response
    MODALITIES:
      - text  # Specifies the modality (e.g., text, image, etc.)
  
  GROQ:
    API_BASE: "https://api.groq.com/openai/v1"
    RESPONSE_MODEL: "llama3-8b-8192"  # Specify the desired Groq model
    TEMPERATURE: 0.7  # Adjust the temperature for response creativity
    TOP_P: 0.9  # Nucleus sampling for diversity
    SYSTEM_PROMPT: "You are a witty and creative assistant."  # Define the system instruction
    MAX_TOKENS: 2000  # Set the maximum number of tokens for the response
    STOP: null  # Define optional stop sequences
    STREAM_OPTIONS:
      include_usage: true  # Include usage details in the response